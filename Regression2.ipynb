{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d27187-412d-4d00-9517-b1e563227246",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5778c6-f872-4e69-953a-7822f29bdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "R-squared (coefficient of determination) is a statistical measure that represents the proportion of the variance in the dependent\n",
    "variable explained by the independent variables in a linear regression model. It ranges from 0 to 1, with higher values indicating\n",
    "a better fit of the model to the data.\n",
    "\n",
    "Mathematically, R-squared is calculated as:\n",
    "\n",
    "[ R^2 = 1 - frac{text{Sum of Squared Residuals}}{\\text{Total Sum of Squares}} ]\n",
    "\n",
    "- Sum of Squared Residuals: The sum of the squared differences between the observed and predicted values.\n",
    "- Total Sum of Squares: The sum of the squared differences between the observed values and the mean of the dependent variable.\n",
    "\n",
    "R-squared values close to 1 indicate that a high percentage of the variability in the dependent variable is explained by the\n",
    "independent variables, suggesting a good fit. Conversely, values close to 0 indicate that the model does not explain much of \n",
    "the variability, suggesting a poor fit. It's important to note that R-squared should be interpreted alongside other metrics\n",
    "and considerations, as a high R-squared does not guarantee a causal relationship or the absence of overfitting.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330ff01-cd18-4292-a007-da45f02e7cb2",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63945d6f-e4b1-4235-b241-6484e1d08982",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of predictors in a linear\n",
    "regression model. While R-squared measures the proportion of variance explained by all the predictors, adjusted R-squared \n",
    "adjusts this value based on the number of predictors, penalizing the inclusion of irrelevant variables.\n",
    "\n",
    "Mathematically, adjusted R-squared is calculated using the formula:\n",
    "\n",
    "[ text{Adjusted } R^2 = 1 - left( frac{(1 - R^2)(n - 1)}{n - k - 1} \\right) ]\n",
    "\n",
    "where:\n",
    "- \\( R^2 \\) is the regular R-squared.\n",
    "- \\( n \\) is the number of observations.\n",
    "- \\( k \\) is the number of predictors.\n",
    "\n",
    "Adjusted R-squared increases only if adding a new predictor improves the model more than would be expected by chance. \n",
    "It provides a more conservative evaluation of model fit, helping to prevent overfitting by penalizing the inclusion of\n",
    "unnecessary variables. Researchers often prefer adjusted R-squared when comparing models with different numbers of predictors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13695c-5337-4d90-b012-47272d8a44fa",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421749e-4e80-45ea-9504-1e22709830a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjusted R-squared is more appropriate when comparing and evaluating models with different numbers of predictors, making it \n",
    "useful in situations where the complexity of the model needs to be considered. Regular R-squared tends to increase as more\n",
    "predictors are added, even if those predictors do not significantly contribute to explaining the variance in the dependent \n",
    "variable. Adjusted R-squared addresses this issue by penalizing models for including irrelevant variables.\n",
    "\n",
    "Researchers and analysts often prefer adjusted R-squared in scenarios where model simplicity and parsimony are important. \n",
    "It helps in selecting models that strike a balance between explaining variance and avoiding overfitting. Adjusted R-squared \n",
    "is particularly valuable when working with regression models with a varying number of predictors or when comparing nested\n",
    "models. Choosing adjusted R-squared over regular R-squared provides a more conservative and realistic measure of a model's\n",
    "goodness of fit.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7dc9d7-95f3-4829-9a0b-fd0f3724cd66",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a71a59-dbff-41d8-a3ce-b4e4a25cd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to assess the accuracy of predictive models.\n",
    "\n",
    "1. RMSE (Root Mean Squared Error): It is calculated by taking the square root of the average of the squared differences\n",
    "        between predicted and actual values. Mathematically, RMSE = (sqrt{frac{sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}).\n",
    "        RMSE provides a measure of the model's overall accuracy, with higher values indicating larger prediction errors.\n",
    "\n",
    "2. MSE (Mean Squared Error): It is the average of the squared differences between predicted and actual values.\n",
    "        MSE = (frac{sum_{i=1}^{n}(y_i - hat{y}_i)^2}{n}). Like RMSE, MSE quantifies the average squared error and is \n",
    "        sensitive to larger errors.\n",
    "\n",
    "3. MAE (Mean Absolute Error): It is the average of the absolute differences between predicted and actual values.\n",
    "        MAE = (frac{sum_{i=1}^{n}|y_i - hat{y}_i|}{n}). MAE is less sensitive to outliers compared to RMSE and MSE, providing a \n",
    "        measure of the average absolute prediction error.\n",
    "\n",
    "Lower values for RMSE, MSE, and MAE indicate better model performance in terms of accuracy and precision.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373b03c-cb53-4ac5-9b31-fd580b0c52d9",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707bb86-ad4d-4f9a-9e49-a0876491e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Advantages:\n",
    "\n",
    "1. RMSE (Root Mean Squared Error):\n",
    "   - Penalizes large errors more heavily, providing a stronger emphasis on significant deviations.\n",
    "   - More sensitive to outliers, making it suitable when large errors are particularly important.\n",
    "\n",
    "2. MSE (Mean Squared Error):\n",
    "   - Easier to compute than RMSE since it doesn't involve the square root operation.\n",
    "   - Emphasizes larger errors, helping to identify substantial deviations from predictions.\n",
    "\n",
    "3. MAE (Mean Absolute Error):\n",
    "   - Robust to outliers as it does not square the errors, providing a more balanced view of overall accuracy.\n",
    "   - Intuitively interpretable, representing the average magnitude of prediction errors.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. RMSE:\n",
    "   - Sensitive to outliers, which can skew the evaluation if the dataset contains extreme values.\n",
    "   - May not be suitable when the emphasis is on smaller errors or when outliers need to be downplayed.\n",
    "\n",
    "2. MSE:\n",
    "   - Similar to RMSE, it can be heavily influenced by outliers, impacting the interpretation of overall model performance.\n",
    "\n",
    "3. MAE:\n",
    "   - Ignores the relative importance of large errors, potentially downplaying their significance.\n",
    "   - Less sensitive to extreme values, which might be crucial in certain applications.\n",
    "\n",
    "Choosing the appropriate metric depends on the specific goals and characteristics of the dataset, with RMSE and MSE often\n",
    "favored when larger errors require more attention, and MAE preferred for a more robust evaluation against outliers.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8f572-b231-45d1-aac5-7356d571d8a6",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115ea11-8b2e-407c-bc4e-b73fa85ac8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lasso regularization, or L1 regularization, is a technique used in linear regression to prevent overfitting and feature \n",
    "selection. It adds a penalty term to the cost function proportional to the absolute values of the regression coefficients. \n",
    "The Lasso objective function is formulated as the sum of the squared residuals and the absolute values of the coefficients\n",
    "multiplied by a regularization parameter (λ).\n",
    "\n",
    "[text{Lasso Objective Function} = text{Sum of Squared Residuals} + lambda sum_{j=1}^{p} |b_j| ]\n",
    "\n",
    "Here, (b_j) represents the regression coefficients, and (p) is the number of predictors.\n",
    "\n",
    "The key difference between Lasso and Ridge regularization (L2 regularization) is the penalty term. While Ridge uses the squared\n",
    "values of the coefficients, Lasso uses their absolute values. This leads Lasso to enforce sparsity, meaning it tends to drive\n",
    "some coefficients exactly to zero, effectively performing feature selection.\n",
    "\n",
    "Lasso is more appropriate when dealing with datasets where many features may be irrelevant or redundant, and a simpler,\n",
    "more interpretable model is desired.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9e673-86ba-4620-885f-ce2fded343a5",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62588eb3-41fc-44b1-9958-dfe6fd9bb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Regularized linear models, such as Lasso and Ridge regression, help prevent overfitting in machine learning by adding a \n",
    "penalty term to the cost function, which discourages excessively complex models with overly large coefficients.\n",
    "This regularization term controls the trade-off between fitting the training data well and keeping the model parameters\n",
    "within reasonable bounds.\n",
    "\n",
    "For example, consider Lasso regression, which adds a penalty term proportional to the absolute values of the regression \n",
    "coefficients. If the model has many features and some are irrelevant, Lasso tends to drive the coefficients of irrelevant\n",
    "features to zero, effectively excluding them from the model. This feature selection property prevents the model from fitting \n",
    "noise in the training data and results in a more generalized model.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754255ef-1553-4d56-aec2-7b096d9c9e6a",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed895f-23cb-463d-a973-9a1c14db7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Regularized linear models, such as Lasso and Ridge regression, have some limitations that may make them less suitable in certain situations:\n",
    "\n",
    "1. Loss of Interpretability: The regularization term can make interpretation of individual coefficients challenging, particularly when features are penalized or excluded.\n",
    "  \n",
    "2. Sensitivity to Outliers: Regularized models may be sensitive to outliers, and the penalty term could be disproportionately influenced by extreme values.\n",
    "  \n",
    "3. Hyperparameter Tuning: The performance of regularized models depends on choosing an appropriate regularization strength(lambda/alpha), which requires careful tuning and validation.\n",
    "\n",
    "4. Data Scaling Sensitivity: Regularized models are sensitive to the scale of the features, and it is often necessary to scale or normalize the data.\n",
    "\n",
    "5. Not Ideal for Every Dataset: In cases where the relationship between features and the target variable is truly linear or \n",
    "when the number of features is small, the added complexity of regularization may not provide significant benefits.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6ed16-e69b-40f8-a3fd-257ee89d824c",
   "metadata": {},
   "source": [
    "# Ans : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5b3e4-3966-4901-b4b6-da6a77ad3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The choice of the better performer depends on the specific goals and characteristics of the problem. If the primary concern \n",
    "is minimizing the impact of larger errors, Model A with an RMSE of 10 may be preferable since RMSE penalizes larger errors \n",
    "more heavily. On the other hand, if the focus is on the average magnitude of errors without giving more weight to outliers, \n",
    "Model B with an MAE of 8 might be preferred.\n",
    "\n",
    "Limitations to consider:\n",
    "1. Sensitivity to Outliers: RMSE is more sensitive to outliers than MAE. If the dataset contains significant outliers, RMSE may be disproportionately influenced.\n",
    "   \n",
    "2. Interpretability: MAE is more interpretable since it represents the average absolute error. RMSE involves a square root operation, which may make interpretation less straightforward.\n",
    "\n",
    "3. Problem-Specific Goals: The choice should align with the specific goals of the analysis. For example, in financial applications, large errors might be more critical than in other domains.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b274e-ce84-4260-b919-5ecf36f1847e",
   "metadata": {},
   "source": [
    "# Ans : 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a08b0-a68e-4687-82f8-841149197763",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The choice between Ridge and Lasso regularization depends on the specific characteristics of the dataset and the goals of the analysis.\n",
    "\n",
    "1. *Model A (Ridge with λ=0.1): Ridge regularization adds a penalty term proportional to the squared values of the coefficients. It is effective in handling multicollinearity and preventing overly large coefficients. A lower regularization parameter (λ=0.1) indicates a milder penalty, allowing for a balance between fitting the data and regularization.\n",
    "\n",
    "2. **Model B (Lasso with λ=0.5):** Lasso regularization includes a penalty term proportional to the absolute values of the coefficients, encouraging sparsity and potentially driving some coefficients to exactly zero. A higher regularization parameter (λ=0.5) suggests a stronger penalty, favoring a more parsimonious model with feature selection.\n",
    "\n",
    "Trade-offs and limitations:\n",
    "- **Ridge:** Suitable when multicollinearity is a concern, and all features are expected to contribute. However, it might not perform well in situations where some features are truly irrelevant.\n",
    "  \n",
    "- **Lasso:** Useful for feature selection, but it tends to choose only one variable among a group of correlated variables. It might not be suitable when all features are expected to contribute.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
